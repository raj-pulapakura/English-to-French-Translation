{"cells":[{"cell_type":"markdown","metadata":{},"source":["# English to French Machine Translation\n","\n","By Raj Pulapakura.\n","\n","- GitHub: https://github.com/raj-pulapakura\n","- Contact: raj.pulapakura@gmail.com\n","\n","### Table of contents:\n","\n","<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->\n","\n","- [1. Load Data](#1-load-data)\n","- [2. Create datasets](#2-create-datasets)\n","- [3. TextVectorization](#3-textvectorization)\n","   * [3.1 Prepare vectorizers](#31-prepare-vectorizers)\n","      + [3.1.1 English Vectorizer](#311-english-vectorizer)\n","      + [3.1.2 French Vectorizer](#312-french-vectorizer)\n","      + [3.1.3 Example from dataset](#313-example-from-dataset)\n","   * [3.2 Create new datasets with word indices](#32-create-new-datasets-with-word-indices)\n","- [4. Building up the Encoder-Decoder Model](#4-building-up-the-encoder-decoder-model)\n","   * [4.1 Encoder](#41-encoder)\n","   * [4.2 Cross-Attention](#42-cross-attention)\n","   * [4.3 Decoder](#43-decoder)\n","   * [4.4 Combining Encoder and Decoder into Translator](#44-combining-encoder-and-decoder-into-translator)\n","- [5. Training](#5-training)\n","- [6. Inference](#6-inference)\n","- [7. Conclusion](#7-conclusion)\n","\n","<!-- TOC end -->\n"]},{"cell_type":"markdown","metadata":{},"source":["### Machine Translation\n","\n","Machine Translation is the process of converting text/speech from one language to another. In this notebook, we tackle specifically translation of English text to French text.\n","\n","### Encoder-Decoder with Attention\n","\n","![Encoder-Decoder architecture with Attention - TensorFlow \"Neural Machine Translation with Attention\" tutorial](https://www.tensorflow.org/images/tutorials/transformer/RNN%2Battention-words-spa.png)\n","\n","`Encoder-Decoder with Attention` is a well-known architecture for machine translation, although it has become somewhat outdated with the rise of the powerful `Transformer` architecture.\n","\n","However, it is still a very useful project to work through to get a deeper understanding of sequence-to-sequence models and attention mechanisms (before going on to Transformers).\n","\n","### Inspiration\n","\n","This notebook was mainly inspired by TensorFlow's amazing tutorial on [Neural machine translation with attention](https://www.tensorflow.org/text/tutorials/nmt_with_attention), which I have made open source contributions to."]},{"cell_type":"markdown","metadata":{},"source":["### Set-up Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:49:48.011203Z","iopub.status.busy":"2023-12-10T01:49:48.010788Z","iopub.status.idle":"2023-12-10T01:49:48.047502Z","shell.execute_reply":"2023-12-10T01:49:48.046166Z","shell.execute_reply.started":"2023-12-10T01:49:48.011172Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-10T01:49:48.050356Z","iopub.status.busy":"2023-12-10T01:49:48.049851Z","iopub.status.idle":"2023-12-10T01:50:03.046739Z","shell.execute_reply":"2023-12-10T01:50:03.045477Z","shell.execute_reply.started":"2023-12-10T01:49:48.050273Z"},"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as tf_text\n","from tensorflow.keras.layers import TextVectorization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:50:03.048811Z","iopub.status.busy":"2023-12-10T01:50:03.048167Z","iopub.status.idle":"2023-12-10T01:50:03.056361Z","shell.execute_reply":"2023-12-10T01:50:03.055428Z","shell.execute_reply.started":"2023-12-10T01:50:03.048777Z"},"trusted":true},"outputs":[],"source":["os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # use cpu"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"1-load-data\"></a>\n","# 1. Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:50:03.060350Z","iopub.status.busy":"2023-12-10T01:50:03.059261Z","iopub.status.idle":"2023-12-10T01:50:03.842052Z","shell.execute_reply":"2023-12-10T01:50:03.841003Z","shell.execute_reply.started":"2023-12-10T01:50:03.060247Z"},"trusted":true},"outputs":[],"source":["data_path = \"/kaggle/input/english-to-french-small-dataset/english_french.csv\"\n","data = pd.read_csv(data_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:50:03.844374Z","iopub.status.busy":"2023-12-10T01:50:03.843667Z","iopub.status.idle":"2023-12-10T01:50:03.867706Z","shell.execute_reply":"2023-12-10T01:50:03.866337Z","shell.execute_reply.started":"2023-12-10T01:50:03.844338Z"},"trusted":true},"outputs":[],"source":["data.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:50:03.869025Z","iopub.status.busy":"2023-12-10T01:50:03.868705Z","iopub.status.idle":"2023-12-10T01:50:03.882267Z","shell.execute_reply":"2023-12-10T01:50:03.881034Z","shell.execute_reply.started":"2023-12-10T01:50:03.868996Z"},"trusted":true},"outputs":[],"source":["data.tail(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:50:03.884626Z","iopub.status.busy":"2023-12-10T01:50:03.883873Z","iopub.status.idle":"2023-12-10T01:50:03.959644Z","shell.execute_reply":"2023-12-10T01:50:03.958369Z","shell.execute_reply.started":"2023-12-10T01:50:03.884589Z"},"trusted":true},"outputs":[],"source":["# Shuffle dataset\n","\n","data = data.sample(frac=1).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:50:03.961850Z","iopub.status.busy":"2023-12-10T01:50:03.961192Z","iopub.status.idle":"2023-12-10T01:50:03.972651Z","shell.execute_reply":"2023-12-10T01:50:03.971510Z","shell.execute_reply.started":"2023-12-10T01:50:03.961814Z"},"trusted":true},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"2-create-datasets\"></a>\n","# 2. Create datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:50:09.803234Z","iopub.status.busy":"2023-12-10T01:50:09.802768Z","iopub.status.idle":"2023-12-10T01:50:09.812344Z","shell.execute_reply":"2023-12-10T01:50:09.810741Z","shell.execute_reply.started":"2023-12-10T01:50:09.803198Z"},"trusted":true},"outputs":[],"source":["test_pct = 0.05\n","n_samples = len(data)\n","n_test = int(n_samples * test_pct)\n","n_train = n_samples - n_test\n","\n","print(f\"Total samples: {n_samples}\")\n","print(f\"Test samples: {n_test}\")\n","print(f\"Train samples: {n_train}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:50:34.312639Z","iopub.status.busy":"2023-12-10T01:50:34.311199Z","iopub.status.idle":"2023-12-10T01:50:34.319691Z","shell.execute_reply":"2023-12-10T01:50:34.318461Z","shell.execute_reply.started":"2023-12-10T01:50:34.312583Z"},"trusted":true},"outputs":[],"source":["english_text = data[\"English\"].to_numpy()\n","french_text = data[\"French\"].to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:50:35.030678Z","iopub.status.busy":"2023-12-10T01:50:35.030146Z","iopub.status.idle":"2023-12-10T01:50:35.291406Z","shell.execute_reply":"2023-12-10T01:50:35.290151Z","shell.execute_reply.started":"2023-12-10T01:50:35.030635Z"},"trusted":true},"outputs":[],"source":["BUFFER_SIZE = 1000\n","BATCH_SIZE = 64\n","\n","ds = tf.data.Dataset.zip(\n","    tf.data.Dataset.from_tensor_slices(english_text),\n","    tf.data.Dataset.from_tensor_slices(french_text)\n",")\n","\n","test_raw = ds.take(n_test).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","train_raw = ds.skip(n_test).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T01:50:37.333494Z","iopub.status.busy":"2023-12-10T01:50:37.332960Z","iopub.status.idle":"2023-12-10T01:50:37.457600Z","shell.execute_reply":"2023-12-10T01:50:37.456394Z","shell.execute_reply.started":"2023-12-10T01:50:37.333452Z"},"trusted":true},"outputs":[],"source":["for english_batch, french_batch in train_raw.take(1):\n","    print(\"English\")\n","    print(english_batch[0:5].numpy())\n","    print(\"\\nFrench\")\n","    print(french_batch[0:5].numpy())"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"3-textvectorization\"></a>\n","# 3. TextVectorization\n","\n","Models don't understand text, so we need to find a way to convert words into numbers.\n","\n","TextVectorization maps each word to an integer. In the process it constructs a vocabulary (dictionary), mapping each word to a unique integer."]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"31-prepare-vectorizers\"></a>\n","## 3.1 Prepare vectorizers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.054447Z","iopub.status.idle":"2023-12-10T01:50:05.054998Z","shell.execute_reply":"2023-12-10T01:50:05.054737Z","shell.execute_reply.started":"2023-12-10T01:50:05.054711Z"},"trusted":true},"outputs":[],"source":["def tf_lower_and_split_punct(text):\n","    \"\"\"\n","    Processes text before vectorization.\n","    \"\"\"\n","    # French text contains special symbols. Unicode normalization:\n","    text = tf_text.normalize_utf8(text, 'NFKD')\n","    # Lowercase\n","    text = tf.strings.lower(text)\n","    # Keep space, a to z, and select punctuation.\n","    text = tf.strings.regex_replace(text, '[^ a-z.?!,Â¿]', '')\n","    # Add spaces around punctuation.\n","    text = tf.strings.regex_replace(text, '[.?!,Â¿]', r' \\0 ')\n","    # Strip whitespace.\n","    text = tf.strings.strip(text)\n","    # start and end tokens\n","    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n","    return text"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"311-english-vectorizer\"></a>\n","### 3.1.1 English Vectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.056654Z","iopub.status.idle":"2023-12-10T01:50:05.057204Z","shell.execute_reply":"2023-12-10T01:50:05.056957Z","shell.execute_reply.started":"2023-12-10T01:50:05.056929Z"},"trusted":true},"outputs":[],"source":["# maximum amount of words in the vocabulary\n","max_vocab_size = 50000 \n","\n","english_vectorizer = TextVectorization(\n","    standardize=tf_lower_and_split_punct,\n","    max_tokens=max_vocab_size, \n","    ragged=True, # ragged=True allows variable length input sequences\n",")\n","\n","# fit vectorization on training dataset english only\n","english_vectorizer.adapt(train_raw.map(lambda english, french: english))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.058616Z","iopub.status.idle":"2023-12-10T01:50:05.059153Z","shell.execute_reply":"2023-12-10T01:50:05.058907Z","shell.execute_reply.started":"2023-12-10T01:50:05.058882Z"},"trusted":true},"outputs":[],"source":["# vectorize example sentence\n","example_sentence = \"Example sentence\"\n","print(f\"Input: {example_sentence}\")\n","print(f\"Vectorized: {english_vectorizer(example_sentence)}\")"]},{"cell_type":"markdown","metadata":{},"source":["The reason there are 4 tokens is because there is a \\<START> token at the start and an \\<END> token at the end."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.060800Z","iopub.status.idle":"2023-12-10T01:50:05.061349Z","shell.execute_reply":"2023-12-10T01:50:05.061083Z","shell.execute_reply.started":"2023-12-10T01:50:05.061057Z"},"trusted":true},"outputs":[],"source":["# get vocabulary size\n","vocab_size = english_vectorizer.vocabulary_size()\n","print(f\"English Vocabulary size: {vocab_size}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.062718Z","iopub.status.idle":"2023-12-10T01:50:05.063259Z","shell.execute_reply":"2023-12-10T01:50:05.063007Z","shell.execute_reply.started":"2023-12-10T01:50:05.062983Z"},"trusted":true},"outputs":[],"source":["# get first 10 words in the English vocabulary\n","print(english_vectorizer.get_vocabulary()[0:10])"]},{"cell_type":"markdown","metadata":{},"source":["Special tokens:\n","\n","- `''` : Padding\n","- `[UNK]` : Unknown token, for words which are not in our vocabulary\n","- `[START]` : Start token, precedes every sentence\n","- `[END]` : End token, succeeds every sentence"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"312-french-vectorizer\"></a>\n","### 3.1.2 French Vectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.064993Z","iopub.status.idle":"2023-12-10T01:50:05.065518Z","shell.execute_reply":"2023-12-10T01:50:05.065281Z","shell.execute_reply.started":"2023-12-10T01:50:05.065255Z"},"trusted":true},"outputs":[],"source":["french_vectorizer = TextVectorization(\n","    standardize=tf_lower_and_split_punct,\n","    max_tokens=max_vocab_size, \n","    ragged=True, # ragged=True allows variable length input sequences\n",")\n","\n","# fit vectorization on training dataset french only\n","french_vectorizer.adapt(train_raw.map(lambda english, french: french))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.067936Z","iopub.status.idle":"2023-12-10T01:50:05.068341Z","shell.execute_reply":"2023-12-10T01:50:05.068150Z","shell.execute_reply.started":"2023-12-10T01:50:05.068132Z"},"trusted":true},"outputs":[],"source":["# vectorize example sentence\n","example_sentence = \"Comment vas-tu?\"\n","print(f\"Input: {example_sentence}\")\n","print(f\"Vectorized: {english_vectorizer(example_sentence)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.070695Z","iopub.status.idle":"2023-12-10T01:50:05.071074Z","shell.execute_reply":"2023-12-10T01:50:05.070905Z","shell.execute_reply.started":"2023-12-10T01:50:05.070887Z"},"trusted":true},"outputs":[],"source":["# get vocabulary size\n","vocab_size = french_vectorizer.vocabulary_size()\n","print(f\"French Vocabulary size: {vocab_size}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.072104Z","iopub.status.idle":"2023-12-10T01:50:05.072496Z","shell.execute_reply":"2023-12-10T01:50:05.072327Z","shell.execute_reply.started":"2023-12-10T01:50:05.072296Z"},"trusted":true},"outputs":[],"source":["# get first 10 words in the French vocabulary\n","print(french_vectorizer.get_vocabulary()[0:10])"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"313-example-from-dataset\"></a>\n","### 3.1.3 Example from dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.073827Z","iopub.status.idle":"2023-12-10T01:50:05.074198Z","shell.execute_reply":"2023-12-10T01:50:05.074031Z","shell.execute_reply.started":"2023-12-10T01:50:05.074014Z"},"trusted":true},"outputs":[],"source":["# take sample from dataset and vectorize\n","\n","for english_b, french_b in train_raw.take(1):\n","    english = english_b[0]\n","    french = french_b[0]\n","    print(\"\\n\\nEnglish (Text)\\n\")\n","    print(english)\n","    print(\"\\n\\nEnglish (Tokens)\\n\")\n","    print(english_vectorizer(english))\n","    print(\"\\n\\nFrench (Text)\\n\")\n","    print(french)\n","    print(\"\\n\\nFrench (Tokens)\\n\")\n","    print(french_vectorizer(french))"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"32-create-new-datasets-with-word-indices\"></a>\n","## 3.2 Create new datasets with word indices"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.075130Z","iopub.status.idle":"2023-12-10T01:50:05.075512Z","shell.execute_reply":"2023-12-10T01:50:05.075347Z","shell.execute_reply.started":"2023-12-10T01:50:05.075329Z"},"trusted":true},"outputs":[],"source":["def process_text(english, french):\n","    \"\"\"\n","    Convert english and french to word indices (tokens).\n","    Extract french_in and french_out from summary.\n","    The difference between french_in and french_out is that they are shifted by one step relative to eachother, so that at each location the label is the next token.\n","    \"\"\"\n","    english_tok = english_vectorizer(english)\n","    french_tok = french_vectorizer(french)\n","    french_tok_in = french_tok[:,:-1]\n","    french_tok_out = french_tok[:, 1:] \n","    return (english_tok, french_tok_in), french_tok_out\n","\n","train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n","test_ds = test_raw.map(process_text, tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.076461Z","iopub.status.idle":"2023-12-10T01:50:05.076822Z","shell.execute_reply":"2023-12-10T01:50:05.076656Z","shell.execute_reply.started":"2023-12-10T01:50:05.076639Z"},"trusted":true},"outputs":[],"source":["for (english_tok, french_in), french_out in train_ds.take(1):\n","    print(\"\\nEnglish tokens:\")\n","    print(english_tok[0, :10].numpy()) \n","    print(\"\\nFrench_in tokens:\")\n","    print(french_in[0, :10].numpy())\n","    print(\"\\nFrench_out tokens (shifted):\")\n","    print(french_out[0, :10].numpy())"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, the `French_out` tokens are equivalent to the `French_in` tokens except they are shifted forward by 1.\n","\n","This automatically creates labels for us, as each token in `French_in` is matched to the following token in `French_out`."]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"4-building-up-the-encoder-decoder-model\"></a>\n","# 4. Building up the Encoder-Decoder Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.077901Z","iopub.status.idle":"2023-12-10T01:50:05.078278Z","shell.execute_reply":"2023-12-10T01:50:05.078101Z","shell.execute_reply.started":"2023-12-10T01:50:05.078084Z"},"trusted":true},"outputs":[],"source":["UNITS = 256"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"41-encoder\"></a>\n","## 4.1 Encoder\n","\n","**Purpose**: Process the english tokens.\n","\n","**Input**: English tokens.\n","\n","**Output**: English encodings.\n","\n","**Steps**:\n","1. Convert English tokens to word embeddings.\n","2. Feed embeddings through Bi-directional RNN.\n","3. Return final English encodings."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.079435Z","iopub.status.idle":"2023-12-10T01:50:05.079807Z","shell.execute_reply":"2023-12-10T01:50:05.079637Z","shell.execute_reply.started":"2023-12-10T01:50:05.079619Z"},"trusted":true},"outputs":[],"source":["class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, vectorizer, units):\n","        super(Encoder, self).__init__()\n","        self.vectorizer = vectorizer\n","        self.vocab_size = vectorizer.vocabulary_size()\n","        self.units = units\n","        \n","        # The embedding layer converts tokens into vectors\n","        self.embedding = tf.keras.layers.Embedding(\n","            input_dim=self.vocab_size,\n","            output_dim=units,\n","        )\n","        \n","        # The RNN layer processes those vectors sequentially\n","        self.rnn = tf.keras.layers.Bidirectional(\n","            merge_mode='sum', # sum forward and backward activation\n","            layer=tf.keras.layers.GRU(\n","                units,\n","                return_sequences=True,\n","                recurrent_initializer='glorot_uniform'\n","            )\n","        )\n","    \n","    def call(self, x):\n","        # 1. The embedding layer looks up the embedding vector for each token.\n","        x = self.embedding(x)\n","        # 2. The GRU processes the sequence of embeddings.\n","        x = self.rnn(x)\n","        # 3. Return the new sequence of embeddings.\n","        return x\n","    \n","    def encode_text(self, texts):\n","        \"\"\"\n","        Converts a list of english texts into encodings\n","        \"\"\"\n","        texts = tf.convert_to_tensor(texts)\n","        if len(texts.shape) == 0:\n","            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n","        tokens = self.vectorizer(texts).to_tensor()\n","        encodings = self(tokens)\n","        return encodings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.080693Z","iopub.status.idle":"2023-12-10T01:50:05.081054Z","shell.execute_reply":"2023-12-10T01:50:05.080888Z","shell.execute_reply.started":"2023-12-10T01:50:05.080872Z"},"trusted":true},"outputs":[],"source":["# Try it out:\n","encoder = Encoder(english_vectorizer, UNITS)\n","\n","# pass example english tokens\n","english_enc = encoder(english_tok)\n","\n","print(f'english tokens, shape (batch, s): {english_tok.shape}')\n","print(f'english encodings, shape (batch, s, units): {english_enc.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["The reason that the shapes contain `None` is because each sentence has a variable length."]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"42-cross-attention\"></a>\n","## 4.2 Cross-Attention\n","\n","**Purpose**: The attention layer lets the decoder access the information extracted by the encoder. It essentially computes contextually aware word embeddings.\n","\n","**Inputs**: English encodings\n","\n","**Outputs**: Attention vectors (contextually aware English encodings)\n","\n","**Steps**: \n","1. Compute Multi-head Attention.\n","2. Add Skip Connection.\n","3. Layer Normalization.\n","4. Return Attention vectors."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.081938Z","iopub.status.idle":"2023-12-10T01:50:05.082329Z","shell.execute_reply":"2023-12-10T01:50:05.082132Z","shell.execute_reply.started":"2023-12-10T01:50:05.082115Z"},"trusted":true},"outputs":[],"source":["class CrossAttention(tf.keras.layers.Layer):\n","    def __init__(self, units, **kwargs):\n","        super().__init__()\n","        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n","        self.layernorm = tf.keras.layers.LayerNormalization()\n","        self.add = tf.keras.layers.Add()\n","\n","    def call(self, french_enc, english_enc):\n","        # compute attention vectors\n","        attn_output, attn_scores = self.mha(\n","            query=french_enc, # query: french encodings\n","            value=english_enc, # value: condition on english encodings\n","            return_attention_scores=True)\n","        \n","        # skip connection to preserve input signals\n","        x = self.add([french_enc, attn_output])\n","        # layer normalization\n","        x = self.layernorm(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.088567Z","iopub.status.idle":"2023-12-10T01:50:05.089032Z","shell.execute_reply":"2023-12-10T01:50:05.088846Z","shell.execute_reply.started":"2023-12-10T01:50:05.088824Z"},"trusted":true},"outputs":[],"source":["# Try it out\n","attention_layer = CrossAttention(UNITS)\n","\n","# simulate French embeddings\n","embed = tf.keras.layers.Embedding(french_vectorizer.vocabulary_size(),\n","                                  output_dim=UNITS)\n","french_embed = embed(french_in)\n","\n","# pass French embeddings and English encodings\n","result = attention_layer(french_embed, english_enc)\n","\n","print(f'English encodings, shape (batch, s, units): {english_enc.shape}')\n","print(f'French embeddings, shape (batch, t, units): {french_embed.shape}')\n","print(f'Attention result, shape (batch, t, units): {result.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"43-decoder\"></a>\n","## 4.3 Decoder\n","\n","**Purpose**: Predict the next token given an input sequence.\n","\n","**Inputs**: English encodings, French input tokens.\n","\n","**Outputs**: Logit predictions for next tokens.\n","\n","**Steps**:\n","1. Convert French tokens to word embeddings.\n","2. Feed word embeddings through Uni-directional RNN.\n","3. Use RNN output as Query for Cross-Attention on English encodings.\n","4. Generate logit predictions for next token."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.091398Z","iopub.status.idle":"2023-12-10T01:50:05.092013Z","shell.execute_reply":"2023-12-10T01:50:05.091837Z","shell.execute_reply.started":"2023-12-10T01:50:05.091817Z"},"trusted":true},"outputs":[],"source":["class Decoder(tf.keras.layers.Layer):\n","    @classmethod\n","    def add_method(cls, fun):\n","        \"\"\"\n","        This will allows us to add additional methods to the class later.\n","        \"\"\"\n","        setattr(cls, fun.__name__, fun)\n","        return fun\n","    \n","    def __init__(self, vectorizer, units):\n","        super(Decoder, self).__init__()\n","        self.vectorizer = vectorizer\n","        self.vocab_size = vectorizer.vocabulary_size()\n","        \n","        self.word_to_id = tf.keras.layers.StringLookup(\n","            vocabulary=vectorizer.get_vocabulary(),\n","            mask_token=\"\", oov_token=\"[UNK]\"\n","        )\n","        \n","        self.id_to_word = tf.keras.layers.StringLookup(\n","            vocabulary=vectorizer.get_vocabulary(),\n","            mask_token=\"\", oov_token=\"[UNK]\",\n","            invert=True\n","        )\n","        \n","        self.start_token = self.word_to_id(\"[START]\")\n","        self.end_token = self.word_to_id(\"[END]\")\n","\n","        # 1. The embedding layer converts token indices to vectors\n","        self.units = units\n","        self.embedding = tf.keras.layers.Embedding(\n","            self.vocab_size,\n","            units,\n","        )\n","\n","        # 2. The RNN keeps track of what's been generated so far\n","        self.rnn = tf.keras.layers.GRU(\n","            units,\n","            return_sequences=True,\n","            return_state=True,\n","            recurrent_initializer=\"glorot_uniform\",\n","        )\n","        \n","        # 3. The RNN output will be the query for the attention layer\n","        self.attention = CrossAttention(units)\n","        \n","        # 4. This fully connected layer produces the logits for each output token\n","        self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n","        \n","    def call(\n","            self, \n","            english_enc, \n","            french_in, \n","            state=None, \n","            return_state=False):\n","        \n","        # 1. Convert french tokens to embeddings\n","        x = self.embedding(french_in)\n","        \n","        # 2. Process the french embeddings\n","        x, state = self.rnn(x, initial_state=state)\n","        \n","        # 3. Use the RNN output as the query for the attention over the english encodings\n","        # Essentially condition the french encodings on the english encodings\n","        x = self.attention(x, english_enc)\n","        \n","        # 4. Generate logit predictions for the next token\n","        logits = self.output_layer(x)\n","        \n","        if return_state:\n","            return logits, state,\n","        else:\n","            return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.093042Z","iopub.status.idle":"2023-12-10T01:50:05.093913Z","shell.execute_reply":"2023-12-10T01:50:05.093717Z","shell.execute_reply.started":"2023-12-10T01:50:05.093693Z"},"trusted":true},"outputs":[],"source":["# Try it out:\n","decoder = Decoder(french_vectorizer, UNITS)\n","\n","# use example English encodings and French input tokens\n","logits = decoder(english_enc, french_in)\n","\n","print(f'English encodings shape (encoder output and decoder input): (batch, s, units) {english_enc.shape}')\n","print(f'French input tokens shape (decoder input): (batch, t) {french_in.shape}')\n","print(f'Logits shape (decoder output): (batch, french_vocabulary_size) {logits.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["Amazing! This is sufficient for training."]},{"cell_type":"markdown","metadata":{},"source":["For inference, we need a couple more methods:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.095357Z","iopub.status.idle":"2023-12-10T01:50:05.096197Z","shell.execute_reply":"2023-12-10T01:50:05.096005Z","shell.execute_reply.started":"2023-12-10T01:50:05.095981Z"},"trusted":true},"outputs":[],"source":["@Decoder.add_method\n","def get_initial_state(self, english_encodings):\n","    batch_size = tf.shape(english_encodings)[0]\n","    # create tensor of n=batch_size start tokens [START]\n","    start_tokens = tf.fill([batch_size, 1], self.start_token)\n","    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n","    embedded = self.embedding(start_tokens)\n","    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.097819Z","iopub.status.idle":"2023-12-10T01:50:05.098231Z","shell.execute_reply":"2023-12-10T01:50:05.098049Z","shell.execute_reply.started":"2023-12-10T01:50:05.098029Z"},"trusted":true},"outputs":[],"source":["@Decoder.add_method\n","def tokens_to_text(self, tokens):\n","    \"\"\"\n","    Convert tokens (word indices) to text\n","    \"\"\"\n","    words = self.id_to_word(tokens)\n","    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n","    result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n","    result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.099757Z","iopub.status.idle":"2023-12-10T01:50:05.100514Z","shell.execute_reply":"2023-12-10T01:50:05.100292Z","shell.execute_reply.started":"2023-12-10T01:50:05.100260Z"},"trusted":true},"outputs":[],"source":["@Decoder.add_method\n","def get_next_token(\n","        self, \n","        english_encodings, \n","        next_token, \n","        done, \n","        state, \n","        temperature=0.0):\n","    \"\"\"\n","    Note: Temperature is a hyperparameter that regulates the randomness or creativity of the AI's responses in language models.\n","    \"\"\"\n","    # running self() automatically runs the call() method\n","    logits, state = self(\n","        english_encodings,\n","        next_token,\n","        state=state,\n","        return_state=True\n","    )\n","    \n","    if temperature == 0.00:\n","        next_token = tf.argmax(logits, axis=-1)\n","    else:\n","        logits = logits[:, -1, :]/temperature\n","        next_token = tf.random.categorical(logits, num_samples=1)\n","        \n","    # if a sequence produces an end_token, set it \"done\"\n","    done = done | (next_token == self.end_token)\n","    # once a sequence is done it only produces 0-padding\n","    next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n","    \n","    return next_token, done, state"]},{"cell_type":"markdown","metadata":{},"source":["With these extra functions, we can write a generation loop."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.101735Z","iopub.status.idle":"2023-12-10T01:50:05.102127Z","shell.execute_reply":"2023-12-10T01:50:05.101952Z","shell.execute_reply.started":"2023-12-10T01:50:05.101933Z"},"trusted":true},"outputs":[],"source":["next_token, done, state = decoder.get_initial_state(english_enc)\n","tokens = []\n","\n","for n in range(10):\n","    # run one step\n","    next_token, done, state = decoder.get_next_token(\n","        english_enc, next_token, done, state, temperature=1.0\n","    )\n","    # add the token to the output\n","    tokens.append(next_token)\n","\n","# stack all the tokens together\n","tokens = tf.concat(tokens, axis=-1) # (batch, t)\n","\n","# Convert the tokens back to strings\n","result = decoder.tokens_to_text(tokens)\n","result"]},{"cell_type":"markdown","metadata":{},"source":["Of course the model is untrained, so the outputs are uniformly random items from the vocabulary."]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"44-combining-encoder-and-decoder-into-translator\"></a>\n","## 4.4 Combining Encoder and Decoder into Translator\n","\n","**Purpose**: Translate English to French.\n","\n","**Inputs**: English tokens, French input tokens.\n","\n","**Outputs**: French translation.\n","\n","**Steps**:\n","1. Feed English tokens through Encoder, generate English encodings.\n","2. Feed English encodings and French input tokens to Decoder, generate prediction logits."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.103288Z","iopub.status.idle":"2023-12-10T01:50:05.103664Z","shell.execute_reply":"2023-12-10T01:50:05.103502Z","shell.execute_reply.started":"2023-12-10T01:50:05.103484Z"},"trusted":true},"outputs":[],"source":["class Translator(tf.keras.Model):\n","    @classmethod\n","    def add_method(cls, fun):\n","        setattr(cls, fun.__name__, fun)\n","        return fun\n","\n","    def __init__(self, units, english_vectorizer, french_vectorizer):\n","        super().__init__()\n","        # build the encoder and decoder\n","        encoder = Encoder(english_vectorizer, units)\n","        decoder = Decoder(french_vectorizer, units)\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        \n","    def call(self, inputs):\n","        # extract english tokens and french input tokens\n","        english_tok, french_in = inputs\n","        # convert english tokens to encodings\n","        english_enc = self.encoder(english_tok)\n","        # compute logits from english encodings and french input tokens\n","        logits = self.decoder(english_enc, french_in)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.104973Z","iopub.status.idle":"2023-12-10T01:50:05.105716Z","shell.execute_reply":"2023-12-10T01:50:05.105526Z","shell.execute_reply.started":"2023-12-10T01:50:05.105505Z"},"trusted":true},"outputs":[],"source":["# Try it out:\n","model = Translator(UNITS, english_vectorizer, french_vectorizer)\n","\n","# pass English tokens and French input tokens\n","logits = model((english_tok, french_in))\n","\n","print(f'English tokens shape (encoder input): (batch, s, units) {english_tok.shape}')\n","print(f'English encodings shape (encoder output and decoder input): (batch, s, units) {english_enc.shape}')\n","print(f'French tokens shape (decoder input): (batch, t) {french_in.shape}')\n","print(f'Logits shape (decoder output): (batch, french_vocabulary_size) {logits.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"5-training\"></a>\n","# 5. Training\n","\n","For training, we need to implement our own masked loss and accuracy functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.106837Z","iopub.status.idle":"2023-12-10T01:50:05.107207Z","shell.execute_reply":"2023-12-10T01:50:05.107039Z","shell.execute_reply.started":"2023-12-10T01:50:05.107022Z"},"trusted":true},"outputs":[],"source":["def masked_loss(y_true, y_pred):\n","    # Calculate the loss for each item in the batch.\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","    loss = loss_fn(y_true, y_pred)\n","\n","    # Mask off the losses on padding.\n","    mask = tf.cast(y_true != 0, loss.dtype)\n","    loss *= mask\n","\n","    # Return the total.\n","    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.108437Z","iopub.status.idle":"2023-12-10T01:50:05.108840Z","shell.execute_reply":"2023-12-10T01:50:05.108659Z","shell.execute_reply.started":"2023-12-10T01:50:05.108640Z"},"trusted":true},"outputs":[],"source":["def masked_acc(y_true, y_pred):\n","    # Calculate the loss for each item in the batch.\n","    y_pred = tf.argmax(y_pred, axis=-1)\n","    y_pred = tf.cast(y_pred, y_true.dtype)\n","\n","    match = tf.cast(y_true == y_pred, tf.float32)\n","    mask = tf.cast(y_true != 0, tf.float32)\n","\n","    return tf.reduce_sum(match)/tf.reduce_sum(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.110271Z","iopub.status.idle":"2023-12-10T01:50:05.110706Z","shell.execute_reply":"2023-12-10T01:50:05.110531Z","shell.execute_reply.started":"2023-12-10T01:50:05.110511Z"},"trusted":true},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss=masked_loss, \n","              metrics=[masked_acc, masked_loss])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.112121Z","iopub.status.idle":"2023-12-10T01:50:05.112541Z","shell.execute_reply":"2023-12-10T01:50:05.112364Z","shell.execute_reply.started":"2023-12-10T01:50:05.112344Z"},"trusted":true},"outputs":[],"source":["vocab_size = 1.0 * french_vectorizer.vocabulary_size()\n","\n","{\n","    \"expected_loss\": tf.math.log(vocab_size).numpy(),\n","    \"expected_acc\": 1/vocab_size\n","}"]},{"cell_type":"markdown","metadata":{},"source":["This should roughly match the values returned by running a few steps of evaluation:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.113826Z","iopub.status.idle":"2023-12-10T01:50:05.115217Z","shell.execute_reply":"2023-12-10T01:50:05.114998Z","shell.execute_reply.started":"2023-12-10T01:50:05.114965Z"},"trusted":true},"outputs":[],"source":["model.evaluate(test_ds, steps=20, return_dict=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.116248Z","iopub.status.idle":"2023-12-10T01:50:05.116669Z","shell.execute_reply":"2023-12-10T01:50:05.116497Z","shell.execute_reply.started":"2023-12-10T01:50:05.116478Z"},"trusted":true},"outputs":[],"source":["history = model.fit(\n","    train_ds.repeat(), # .repeat() makes it an infinite dataset\n","    validation_data=test_ds,\n","    epochs=20,\n","    steps_per_epoch = 100, # since we are using an infinite dataset, we need to specify the number of steps per epoch\n","    validation_steps = 20,\n","    callbacks=[\n","        tf.keras.callbacks.EarlyStopping(patience=3)\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"6-inference\"></a>\n","# 6. Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.117879Z","iopub.status.idle":"2023-12-10T01:50:05.118272Z","shell.execute_reply":"2023-12-10T01:50:05.118101Z","shell.execute_reply.started":"2023-12-10T01:50:05.118082Z"},"trusted":true},"outputs":[],"source":["@Translator.add_method\n","def translate(self,\n","              texts, *,\n","              max_length=50,\n","              temperature=0.0):\n","    # Process the input texts\n","    context = self.encoder.encode_text(texts)\n","    batch_size = tf.shape(texts)[0]\n","\n","    # Setup the loop inputs\n","    tokens = []\n","    next_token, done, state = self.decoder.get_initial_state(context)\n","\n","    for _ in range(max_length):\n","        # Generate the next token\n","        next_token, done, state = self.decoder.get_next_token(context, next_token, done,  state, temperature)\n","\n","        # Collect the generated tokens\n","        tokens.append(next_token)\n","\n","        if tf.executing_eagerly() and tf.reduce_all(done):\n","            break\n","\n","    # Stack the lists of tokens and attention weights.\n","    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n","\n","    result = self.decoder.tokens_to_text(tokens)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.122580Z","iopub.status.idle":"2023-12-10T01:50:05.123014Z","shell.execute_reply":"2023-12-10T01:50:05.122832Z","shell.execute_reply.started":"2023-12-10T01:50:05.122812Z"},"trusted":true},"outputs":[],"source":["# Try it out:\n","result = model.translate([\"This is a wonderful day\"]) # Câ€™est un jour merveilleux\n","result[0].numpy().decode()"]},{"cell_type":"markdown","metadata":{},"source":["<!-- TOC --><a name=\"7-conclusion\"></a>\n","# 7. Conclusion\n","\n","In this notebook, we used the Encoder-Decoder architecture with Attention to translate English text to French text.\n","\n","ðŸ˜Š If you enjoyed this notebook or found it inspiring/useful, an upvote would be really appreciated."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4110621,"sourceId":7164589,"sourceType":"datasetVersion"},{"modelInstanceId":2200,"sourceId":2961,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
