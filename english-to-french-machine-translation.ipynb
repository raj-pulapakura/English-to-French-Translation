{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7164589,"sourceType":"datasetVersion","datasetId":4110621},{"sourceId":2961,"sourceType":"modelInstanceVersion","modelInstanceId":2200}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# English to French Machine Translation\n\nBy Raj Pulapakura.\n\n- GitHub: https://github.com/raj-pulapakura\n- Contact: raj.pulapakura@gmail.com\n\n### Table of contents:\n\n<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->\n\n- [1. Load Data](#1-load-data)\n- [2. Create datasets](#2-create-datasets)\n- [3. TextVectorization](#3-textvectorization)\n   * [3.1 Prepare vectorizers](#31-prepare-vectorizers)\n      + [3.1.1 English Vectorizer](#311-english-vectorizer)\n      + [3.1.2 French Vectorizer](#312-french-vectorizer)\n      + [3.1.3 Example from dataset](#313-example-from-dataset)\n   * [3.2 Create new datasets with word indices](#32-create-new-datasets-with-word-indices)\n- [4. Building up the Encoder-Decoder Model](#4-building-up-the-encoder-decoder-model)\n   * [4.1 Encoder](#41-encoder)\n   * [4.2 Cross-Attention](#42-cross-attention)\n   * [4.3 Decoder](#43-decoder)\n   * [4.4 Combining Encoder and Decoder into Translator](#44-combining-encoder-and-decoder-into-translator)\n- [5. Training](#5-training)\n- [6. Inference](#6-inference)\n- [7. Conclusion](#7-conclusion)\n\n<!-- TOC end -->\n","metadata":{}},{"cell_type":"markdown","source":"### Machine Translation\n\nMachine Translation is the process of converting text/speech from one language to another. In this notebook, we tackle specifically translation of English text to French text.\n\n### Encoder-Decoder with Attention\n\n`Encoder-Decoder with Attention` is a well-known architecture for machine translation, although it has become somewhat outdated with the rise of the powerful `Transformer` architecture.\n\nHowever, it is still a very useful project to work through to get a deeper understanding of sequence-to-sequence models and attention mechanisms (before going on to Transformers).\n\n### Inspiration\n\nThis notebook was mainly inspired by TensorFlow's amazing tutorial on [Neural machine translation with attention](https://www.tensorflow.org/text/tutorials/nmt_with_attention), which I have made open source contributions to.","metadata":{}},{"cell_type":"markdown","source":"### Set-up Notebook","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:49:48.010788Z","iopub.execute_input":"2023-12-10T01:49:48.011203Z","iopub.status.idle":"2023-12-10T01:49:48.047502Z","shell.execute_reply.started":"2023-12-10T01:49:48.011172Z","shell.execute_reply":"2023-12-10T01:49:48.046166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as tf_text\nfrom tensorflow.keras.layers import TextVectorization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-10T01:49:48.049851Z","iopub.execute_input":"2023-12-10T01:49:48.050356Z","iopub.status.idle":"2023-12-10T01:50:03.046739Z","shell.execute_reply.started":"2023-12-10T01:49:48.050273Z","shell.execute_reply":"2023-12-10T01:50:03.045477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # use cpu","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:03.048167Z","iopub.execute_input":"2023-12-10T01:50:03.048811Z","iopub.status.idle":"2023-12-10T01:50:03.056361Z","shell.execute_reply.started":"2023-12-10T01:50:03.048777Z","shell.execute_reply":"2023-12-10T01:50:03.055428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"1-load-data\"></a>\n# 1. Load Data","metadata":{}},{"cell_type":"code","source":"data_path = \"/kaggle/input/english-to-french-small-dataset/english_french.csv\"\ndata = pd.read_csv(data_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:03.059261Z","iopub.execute_input":"2023-12-10T01:50:03.060350Z","iopub.status.idle":"2023-12-10T01:50:03.842052Z","shell.execute_reply.started":"2023-12-10T01:50:03.060247Z","shell.execute_reply":"2023-12-10T01:50:03.841003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:03.843667Z","iopub.execute_input":"2023-12-10T01:50:03.844374Z","iopub.status.idle":"2023-12-10T01:50:03.867706Z","shell.execute_reply.started":"2023-12-10T01:50:03.844338Z","shell.execute_reply":"2023-12-10T01:50:03.866337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.tail(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:03.868705Z","iopub.execute_input":"2023-12-10T01:50:03.869025Z","iopub.status.idle":"2023-12-10T01:50:03.882267Z","shell.execute_reply.started":"2023-12-10T01:50:03.868996Z","shell.execute_reply":"2023-12-10T01:50:03.881034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffle dataset\n\ndata = data.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:03.883873Z","iopub.execute_input":"2023-12-10T01:50:03.884626Z","iopub.status.idle":"2023-12-10T01:50:03.959644Z","shell.execute_reply.started":"2023-12-10T01:50:03.884589Z","shell.execute_reply":"2023-12-10T01:50:03.958369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:03.961192Z","iopub.execute_input":"2023-12-10T01:50:03.961850Z","iopub.status.idle":"2023-12-10T01:50:03.972651Z","shell.execute_reply.started":"2023-12-10T01:50:03.961814Z","shell.execute_reply":"2023-12-10T01:50:03.971510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"2-create-datasets\"></a>\n# 2. Create datasets","metadata":{}},{"cell_type":"code","source":"test_pct = 0.05\nn_samples = len(data)\nn_test = int(n_samples * test_pct)\nn_train = n_samples - n_test\n\nprint(f\"Total samples: {n_samples}\")\nprint(f\"Test samples: {n_test}\")\nprint(f\"Train samples: {n_train}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:09.802768Z","iopub.execute_input":"2023-12-10T01:50:09.803234Z","iopub.status.idle":"2023-12-10T01:50:09.812344Z","shell.execute_reply.started":"2023-12-10T01:50:09.803198Z","shell.execute_reply":"2023-12-10T01:50:09.810741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english_text = data[\"English\"].to_numpy()\nfrench_text = data[\"French\"].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:34.311199Z","iopub.execute_input":"2023-12-10T01:50:34.312639Z","iopub.status.idle":"2023-12-10T01:50:34.319691Z","shell.execute_reply.started":"2023-12-10T01:50:34.312583Z","shell.execute_reply":"2023-12-10T01:50:34.318461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 1000\nBATCH_SIZE = 64\n\nds = tf.data.Dataset.zip(\n    tf.data.Dataset.from_tensor_slices(english_text),\n    tf.data.Dataset.from_tensor_slices(french_text)\n)\n\ntest_raw = ds.take(n_test).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ntrain_raw = ds.skip(n_test).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:35.030146Z","iopub.execute_input":"2023-12-10T01:50:35.030678Z","iopub.status.idle":"2023-12-10T01:50:35.291406Z","shell.execute_reply.started":"2023-12-10T01:50:35.030635Z","shell.execute_reply":"2023-12-10T01:50:35.290151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for english_batch, french_batch in train_raw.take(1):\n    print(\"English\")\n    print(english_batch[0:5].numpy())\n    print(\"\\nFrench\")\n    print(french_batch[0:5].numpy())","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:37.332960Z","iopub.execute_input":"2023-12-10T01:50:37.333494Z","iopub.status.idle":"2023-12-10T01:50:37.457600Z","shell.execute_reply.started":"2023-12-10T01:50:37.333452Z","shell.execute_reply":"2023-12-10T01:50:37.456394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"3-textvectorization\"></a>\n# 3. TextVectorization\n\nModels don't understand text, so we need to find a way to convert words into numbers.\n\nTextVectorization maps each word to an integer. In the process it constructs a vocabulary (dictionary), mapping each word to a unique integer.","metadata":{}},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"31-prepare-vectorizers\"></a>\n## 3.1 Prepare vectorizers","metadata":{}},{"cell_type":"code","source":"def tf_lower_and_split_punct(text):\n    \"\"\"\n    Processes text before vectorization.\n    \"\"\"\n    # French text contains special symbols. Unicode normalization:\n    text = tf_text.normalize_utf8(text, 'NFKD')\n    # Lowercase\n    text = tf.strings.lower(text)\n    # Keep space, a to z, and select punctuation.\n    text = tf.strings.regex_replace(text, '[^ a-z.?!,Â¿]', '')\n    # Add spaces around punctuation.\n    text = tf.strings.regex_replace(text, '[.?!,Â¿]', r' \\0 ')\n    # Strip whitespace.\n    text = tf.strings.strip(text)\n    # start and end tokens\n    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.054447Z","iopub.status.idle":"2023-12-10T01:50:05.054998Z","shell.execute_reply.started":"2023-12-10T01:50:05.054711Z","shell.execute_reply":"2023-12-10T01:50:05.054737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"311-english-vectorizer\"></a>\n### 3.1.1 English Vectorizer","metadata":{}},{"cell_type":"code","source":"# maximum amount of words in the vocabulary\nmax_vocab_size = 50000 \n\nenglish_vectorizer = TextVectorization(\n    standardize=tf_lower_and_split_punct,\n    max_tokens=max_vocab_size, \n    ragged=True, # ragged=True allows variable length input sequences\n)\n\n# fit vectorization on training dataset english only\nenglish_vectorizer.adapt(train_raw.map(lambda english, french: english))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.056654Z","iopub.status.idle":"2023-12-10T01:50:05.057204Z","shell.execute_reply.started":"2023-12-10T01:50:05.056929Z","shell.execute_reply":"2023-12-10T01:50:05.056957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vectorize example sentence\nexample_sentence = \"Example sentence\"\nprint(f\"Input: {example_sentence}\")\nprint(f\"Vectorized: {english_vectorizer(example_sentence)}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.058616Z","iopub.status.idle":"2023-12-10T01:50:05.059153Z","shell.execute_reply.started":"2023-12-10T01:50:05.058882Z","shell.execute_reply":"2023-12-10T01:50:05.058907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The reason there are 4 tokens is because there is a \\<START> token at the start and an \\<END> token at the end.","metadata":{}},{"cell_type":"code","source":"# get vocabulary size\nvocab_size = english_vectorizer.vocabulary_size()\nprint(f\"English Vocabulary size: {vocab_size}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.060800Z","iopub.status.idle":"2023-12-10T01:50:05.061349Z","shell.execute_reply.started":"2023-12-10T01:50:05.061057Z","shell.execute_reply":"2023-12-10T01:50:05.061083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get first 10 words in the English vocabulary\nprint(english_vectorizer.get_vocabulary()[0:10])","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.062718Z","iopub.status.idle":"2023-12-10T01:50:05.063259Z","shell.execute_reply.started":"2023-12-10T01:50:05.062983Z","shell.execute_reply":"2023-12-10T01:50:05.063007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Special tokens:\n\n- `''` : Padding\n- `[UNK]` : Unknown token, for words which are not in our vocabulary\n- `[START]` : Start token, precedes every sentence\n- `[END]` : End token, succeeds every sentence","metadata":{}},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"312-french-vectorizer\"></a>\n### 3.1.2 French Vectorizer","metadata":{}},{"cell_type":"code","source":"french_vectorizer = TextVectorization(\n    standardize=tf_lower_and_split_punct,\n    max_tokens=max_vocab_size, \n    ragged=True, # ragged=True allows variable length input sequences\n)\n\n# fit vectorization on training dataset french only\nfrench_vectorizer.adapt(train_raw.map(lambda english, french: french))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.064993Z","iopub.status.idle":"2023-12-10T01:50:05.065518Z","shell.execute_reply.started":"2023-12-10T01:50:05.065255Z","shell.execute_reply":"2023-12-10T01:50:05.065281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vectorize example sentence\nexample_sentence = \"Comment vas-tu?\"\nprint(f\"Input: {example_sentence}\")\nprint(f\"Vectorized: {english_vectorizer(example_sentence)}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.067936Z","iopub.status.idle":"2023-12-10T01:50:05.068341Z","shell.execute_reply.started":"2023-12-10T01:50:05.068132Z","shell.execute_reply":"2023-12-10T01:50:05.068150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get vocabulary size\nvocab_size = french_vectorizer.vocabulary_size()\nprint(f\"French Vocabulary size: {vocab_size}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.070695Z","iopub.status.idle":"2023-12-10T01:50:05.071074Z","shell.execute_reply.started":"2023-12-10T01:50:05.070887Z","shell.execute_reply":"2023-12-10T01:50:05.070905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get first 10 words in the French vocabulary\nprint(french_vectorizer.get_vocabulary()[0:10])","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.072104Z","iopub.status.idle":"2023-12-10T01:50:05.072496Z","shell.execute_reply.started":"2023-12-10T01:50:05.072296Z","shell.execute_reply":"2023-12-10T01:50:05.072327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"313-example-from-dataset\"></a>\n### 3.1.3 Example from dataset","metadata":{}},{"cell_type":"code","source":"# take sample from dataset and vectorize\n\nfor english_b, french_b in train_raw.take(1):\n    english = english_b[0]\n    french = french_b[0]\n    print(\"\\n\\nEnglish (Text)\\n\")\n    print(english)\n    print(\"\\n\\nEnglish (Tokens)\\n\")\n    print(english_vectorizer(english))\n    print(\"\\n\\nFrench (Text)\\n\")\n    print(french)\n    print(\"\\n\\nFrench (Tokens)\\n\")\n    print(french_vectorizer(french))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.073827Z","iopub.status.idle":"2023-12-10T01:50:05.074198Z","shell.execute_reply.started":"2023-12-10T01:50:05.074014Z","shell.execute_reply":"2023-12-10T01:50:05.074031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"32-create-new-datasets-with-word-indices\"></a>\n## 3.2 Create new datasets with word indices","metadata":{}},{"cell_type":"code","source":"def process_text(english, french):\n    \"\"\"\n    Convert english and french to word indices (tokens).\n    Extract french_in and french_out from summary.\n    The difference between french_in and french_out is that they are shifted by one step relative to eachother, so that at each location the label is the next token.\n    \"\"\"\n    english_tok = english_vectorizer(english)\n    french_tok = french_vectorizer(french)\n    french_tok_in = french_tok[:,:-1]\n    french_tok_out = french_tok[:, 1:] \n    return (english_tok, french_tok_in), french_tok_out\n\ntrain_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\ntest_ds = test_raw.map(process_text, tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.075130Z","iopub.status.idle":"2023-12-10T01:50:05.075512Z","shell.execute_reply.started":"2023-12-10T01:50:05.075329Z","shell.execute_reply":"2023-12-10T01:50:05.075347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (english_tok, french_in), french_out in train_ds.take(1):\n    print(\"\\nEnglish tokens:\")\n    print(english_tok[0, :10].numpy()) \n    print(\"\\nFrench_in tokens:\")\n    print(french_in[0, :10].numpy())\n    print(\"\\nFrench_out tokens (shifted):\")\n    print(french_out[0, :10].numpy())","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.076461Z","iopub.status.idle":"2023-12-10T01:50:05.076822Z","shell.execute_reply.started":"2023-12-10T01:50:05.076639Z","shell.execute_reply":"2023-12-10T01:50:05.076656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the `French_out` tokens are equivalent to the `French_in` tokens except they are shifted forward by 1.\n\nThis automatically creates labels for us, as each token in `French_in` is matched to the following token in `French_out`.","metadata":{}},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"4-building-up-the-encoder-decoder-model\"></a>\n# 4. Building up the Encoder-Decoder Model","metadata":{}},{"cell_type":"code","source":"UNITS = 256","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.077901Z","iopub.status.idle":"2023-12-10T01:50:05.078278Z","shell.execute_reply.started":"2023-12-10T01:50:05.078084Z","shell.execute_reply":"2023-12-10T01:50:05.078101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"41-encoder\"></a>\n## 4.1 Encoder\n\n**Purpose**: Process the english tokens.\n\n**Input**: English tokens.\n\n**Output**: English encodings.\n\n**Steps**:\n1. Convert English tokens to word embeddings.\n2. Feed embeddings through Bi-directional RNN.\n3. Return final English encodings.","metadata":{}},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, vectorizer, units):\n        super(Encoder, self).__init__()\n        self.vectorizer = vectorizer\n        self.vocab_size = vectorizer.vocabulary_size()\n        self.units = units\n        \n        # The embedding layer converts tokens into vectors\n        self.embedding = tf.keras.layers.Embedding(\n            input_dim=self.vocab_size,\n            output_dim=units,\n        )\n        \n        # The RNN layer processes those vectors sequentially\n        self.rnn = tf.keras.layers.Bidirectional(\n            merge_mode='sum', # sum forward and backward activation\n            layer=tf.keras.layers.GRU(\n                units,\n                return_sequences=True,\n                recurrent_initializer='glorot_uniform'\n            )\n        )\n    \n    def call(self, x):\n        # 1. The embedding layer looks up the embedding vector for each token.\n        x = self.embedding(x)\n        # 2. The GRU processes the sequence of embeddings.\n        x = self.rnn(x)\n        # 3. Return the new sequence of embeddings.\n        return x\n    \n    def encode_text(self, texts):\n        \"\"\"\n        Converts a list of english texts into encodings\n        \"\"\"\n        texts = tf.convert_to_tensor(texts)\n        if len(texts.shape) == 0:\n            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n        tokens = self.vectorizer(texts).to_tensor()\n        encodings = self(tokens)\n        return encodings","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.079435Z","iopub.status.idle":"2023-12-10T01:50:05.079807Z","shell.execute_reply.started":"2023-12-10T01:50:05.079619Z","shell.execute_reply":"2023-12-10T01:50:05.079637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try it out:\nencoder = Encoder(english_vectorizer, UNITS)\n\n# pass example english tokens\nenglish_enc = encoder(english_tok)\n\nprint(f'english tokens, shape (batch, s): {english_tok.shape}')\nprint(f'english encodings, shape (batch, s, units): {english_enc.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.080693Z","iopub.status.idle":"2023-12-10T01:50:05.081054Z","shell.execute_reply.started":"2023-12-10T01:50:05.080872Z","shell.execute_reply":"2023-12-10T01:50:05.080888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The reason that the shapes contain `None` is because each sentence has a variable length.","metadata":{}},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"42-cross-attention\"></a>\n## 4.2 Cross-Attention\n\n**Purpose**: The attention layer lets the decoder access the information extracted by the encoder. It essentially computes contextually aware word embeddings.\n\n**Inputs**: English encodings\n\n**Outputs**: Attention vectors (contextually aware English encodings)\n\n**Steps**: \n1. Compute Multi-head Attention.\n2. Add Skip Connection.\n3. Layer Normalization.\n4. Return Attention vectors.","metadata":{}},{"cell_type":"code","source":"class CrossAttention(tf.keras.layers.Layer):\n    def __init__(self, units, **kwargs):\n        super().__init__()\n        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n        self.layernorm = tf.keras.layers.LayerNormalization()\n        self.add = tf.keras.layers.Add()\n\n    def call(self, french_enc, english_enc):\n        # compute attention vectors\n        attn_output, attn_scores = self.mha(\n            query=french_enc, # query: french encodings\n            value=english_enc, # value: condition on english encodings\n            return_attention_scores=True)\n        \n        # skip connection to preserve input signals\n        x = self.add([french_enc, attn_output])\n        # layer normalization\n        x = self.layernorm(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.081938Z","iopub.status.idle":"2023-12-10T01:50:05.082329Z","shell.execute_reply.started":"2023-12-10T01:50:05.082115Z","shell.execute_reply":"2023-12-10T01:50:05.082132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try it out\nattention_layer = CrossAttention(UNITS)\n\n# simulate French embeddings\nembed = tf.keras.layers.Embedding(french_vectorizer.vocabulary_size(),\n                                  output_dim=UNITS)\nfrench_embed = embed(french_in)\n\n# pass French embeddings and English encodings\nresult = attention_layer(french_embed, english_enc)\n\nprint(f'English encodings, shape (batch, s, units): {english_enc.shape}')\nprint(f'French embeddings, shape (batch, t, units): {french_embed.shape}')\nprint(f'Attention result, shape (batch, t, units): {result.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.088567Z","iopub.status.idle":"2023-12-10T01:50:05.089032Z","shell.execute_reply.started":"2023-12-10T01:50:05.088824Z","shell.execute_reply":"2023-12-10T01:50:05.088846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"43-decoder\"></a>\n## 4.3 Decoder\n\n**Purpose**: Predict the next token given an input sequence.\n\n**Inputs**: English encodings, French input tokens.\n\n**Outputs**: Logit predictions for next tokens.\n\n**Steps**:\n1. Convert French tokens to word embeddings.\n2. Feed word embeddings through Uni-directional RNN.\n3. Use RNN output as Query for Cross-Attention on English encodings.\n4. Generate logit predictions for next token.","metadata":{}},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    @classmethod\n    def add_method(cls, fun):\n        \"\"\"\n        This will allows us to add additional methods to the class later.\n        \"\"\"\n        setattr(cls, fun.__name__, fun)\n        return fun\n    \n    def __init__(self, vectorizer, units):\n        super(Decoder, self).__init__()\n        self.vectorizer = vectorizer\n        self.vocab_size = vectorizer.vocabulary_size()\n        \n        self.word_to_id = tf.keras.layers.StringLookup(\n            vocabulary=vectorizer.get_vocabulary(),\n            mask_token=\"\", oov_token=\"[UNK]\"\n        )\n        \n        self.id_to_word = tf.keras.layers.StringLookup(\n            vocabulary=vectorizer.get_vocabulary(),\n            mask_token=\"\", oov_token=\"[UNK]\",\n            invert=True\n        )\n        \n        self.start_token = self.word_to_id(\"[START]\")\n        self.end_token = self.word_to_id(\"[END]\")\n\n        # 1. The embedding layer converts token indices to vectors\n        self.units = units\n        self.embedding = tf.keras.layers.Embedding(\n            self.vocab_size,\n            units,\n        )\n\n        # 2. The RNN keeps track of what's been generated so far\n        self.rnn = tf.keras.layers.GRU(\n            units,\n            return_sequences=True,\n            return_state=True,\n            recurrent_initializer=\"glorot_uniform\",\n        )\n        \n        # 3. The RNN output will be the query for the attention layer\n        self.attention = CrossAttention(units)\n        \n        # 4. This fully connected layer produces the logits for each output token\n        self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n        \n    def call(\n            self, \n            english_enc, \n            french_in, \n            state=None, \n            return_state=False):\n        \n        # 1. Convert french tokens to embeddings\n        x = self.embedding(french_in)\n        \n        # 2. Process the french embeddings\n        x, state = self.rnn(x, initial_state=state)\n        \n        # 3. Use the RNN output as the query for the attention over the english encodings\n        # Essentially condition the french encodings on the english encodings\n        x = self.attention(x, english_enc)\n        \n        # 4. Generate logit predictions for the next token\n        logits = self.output_layer(x)\n        \n        if return_state:\n            return logits, state,\n        else:\n            return logits","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.091398Z","iopub.status.idle":"2023-12-10T01:50:05.092013Z","shell.execute_reply.started":"2023-12-10T01:50:05.091817Z","shell.execute_reply":"2023-12-10T01:50:05.091837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try it out:\ndecoder = Decoder(french_vectorizer, UNITS)\n\n# use example English encodings and French input tokens\nlogits = decoder(english_enc, french_in)\n\nprint(f'English encodings shape (encoder output and decoder input): (batch, s, units) {english_enc.shape}')\nprint(f'French input tokens shape (decoder input): (batch, t) {french_in.shape}')\nprint(f'Logits shape (decoder output): (batch, french_vocabulary_size) {logits.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.093042Z","iopub.status.idle":"2023-12-10T01:50:05.093913Z","shell.execute_reply.started":"2023-12-10T01:50:05.093693Z","shell.execute_reply":"2023-12-10T01:50:05.093717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Amazing! This is sufficient for training.","metadata":{}},{"cell_type":"markdown","source":"For inference, we need a couple more methods:","metadata":{}},{"cell_type":"code","source":"@Decoder.add_method\ndef get_initial_state(self, english_encodings):\n    batch_size = tf.shape(english_encodings)[0]\n    # create tensor of n=batch_size start tokens [START]\n    start_tokens = tf.fill([batch_size, 1], self.start_token)\n    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n    embedded = self.embedding(start_tokens)\n    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.095357Z","iopub.status.idle":"2023-12-10T01:50:05.096197Z","shell.execute_reply.started":"2023-12-10T01:50:05.095981Z","shell.execute_reply":"2023-12-10T01:50:05.096005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@Decoder.add_method\ndef tokens_to_text(self, tokens):\n    \"\"\"\n    Convert tokens (word indices) to text\n    \"\"\"\n    words = self.id_to_word(tokens)\n    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n    result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n    result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.097819Z","iopub.status.idle":"2023-12-10T01:50:05.098231Z","shell.execute_reply.started":"2023-12-10T01:50:05.098029Z","shell.execute_reply":"2023-12-10T01:50:05.098049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@Decoder.add_method\ndef get_next_token(\n        self, \n        english_encodings, \n        next_token, \n        done, \n        state, \n        temperature=0.0):\n    \"\"\"\n    Note: Temperature is a hyperparameter that regulates the randomness or creativity of the AI's responses in language models.\n    \"\"\"\n    # running self() automatically runs the call() method\n    logits, state = self(\n        english_encodings,\n        next_token,\n        state=state,\n        return_state=True\n    )\n    \n    if temperature == 0.00:\n        next_token = tf.argmax(logits, axis=-1)\n    else:\n        logits = logits[:, -1, :]/temperature\n        next_token = tf.random.categorical(logits, num_samples=1)\n        \n    # if a sequence produces an end_token, set it \"done\"\n    done = done | (next_token == self.end_token)\n    # once a sequence is done it only produces 0-padding\n    next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n    \n    return next_token, done, state","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.099757Z","iopub.status.idle":"2023-12-10T01:50:05.100514Z","shell.execute_reply.started":"2023-12-10T01:50:05.100260Z","shell.execute_reply":"2023-12-10T01:50:05.100292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With these extra functions, we can write a generation loop.","metadata":{}},{"cell_type":"code","source":"next_token, done, state = decoder.get_initial_state(english_enc)\ntokens = []\n\nfor n in range(10):\n    # run one step\n    next_token, done, state = decoder.get_next_token(\n        english_enc, next_token, done, state, temperature=1.0\n    )\n    # add the token to the output\n    tokens.append(next_token)\n\n# stack all the tokens together\ntokens = tf.concat(tokens, axis=-1) # (batch, t)\n\n# Convert the tokens back to strings\nresult = decoder.tokens_to_text(tokens)\nresult","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.101735Z","iopub.status.idle":"2023-12-10T01:50:05.102127Z","shell.execute_reply.started":"2023-12-10T01:50:05.101933Z","shell.execute_reply":"2023-12-10T01:50:05.101952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of course the model is untrained, so the outputs are uniformly random items from the vocabulary.","metadata":{}},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"44-combining-encoder-and-decoder-into-translator\"></a>\n## 4.4 Combining Encoder and Decoder into Translator\n\n**Purpose**: Translate English to French.\n\n**Inputs**: English tokens, French input tokens.\n\n**Outputs**: French translation.\n\n**Steps**:\n1. Feed English tokens through Encoder, generate English encodings.\n2. Feed English encodings and French input tokens to Decoder, generate prediction logits.","metadata":{}},{"cell_type":"code","source":"class Translator(tf.keras.Model):\n    @classmethod\n    def add_method(cls, fun):\n        setattr(cls, fun.__name__, fun)\n        return fun\n\n    def __init__(self, units, english_vectorizer, french_vectorizer):\n        super().__init__()\n        # build the encoder and decoder\n        encoder = Encoder(english_vectorizer, units)\n        decoder = Decoder(french_vectorizer, units)\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def call(self, inputs):\n        # extract english tokens and french input tokens\n        english_tok, french_in = inputs\n        # convert english tokens to encodings\n        english_enc = self.encoder(english_tok)\n        # compute logits from english encodings and french input tokens\n        logits = self.decoder(english_enc, french_in)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.103288Z","iopub.status.idle":"2023-12-10T01:50:05.103664Z","shell.execute_reply.started":"2023-12-10T01:50:05.103484Z","shell.execute_reply":"2023-12-10T01:50:05.103502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try it out:\nmodel = Translator(UNITS, english_vectorizer, french_vectorizer)\n\n# pass English tokens and French input tokens\nlogits = model((english_tok, french_in))\n\nprint(f'English tokens shape (encoder input): (batch, s, units) {english_tok.shape}')\nprint(f'English encodings shape (encoder output and decoder input): (batch, s, units) {english_enc.shape}')\nprint(f'French tokens shape (decoder input): (batch, t) {french_in.shape}')\nprint(f'Logits shape (decoder output): (batch, french_vocabulary_size) {logits.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.104973Z","iopub.status.idle":"2023-12-10T01:50:05.105716Z","shell.execute_reply.started":"2023-12-10T01:50:05.105505Z","shell.execute_reply":"2023-12-10T01:50:05.105526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"5-training\"></a>\n# 5. Training\n\nFor training, we need to implement our own masked loss and accuracy functions:","metadata":{}},{"cell_type":"code","source":"def masked_loss(y_true, y_pred):\n    # Calculate the loss for each item in the batch.\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction='none')\n    loss = loss_fn(y_true, y_pred)\n\n    # Mask off the losses on padding.\n    mask = tf.cast(y_true != 0, loss.dtype)\n    loss *= mask\n\n    # Return the total.\n    return tf.reduce_sum(loss)/tf.reduce_sum(mask)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.106837Z","iopub.status.idle":"2023-12-10T01:50:05.107207Z","shell.execute_reply.started":"2023-12-10T01:50:05.107022Z","shell.execute_reply":"2023-12-10T01:50:05.107039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def masked_acc(y_true, y_pred):\n    # Calculate the loss for each item in the batch.\n    y_pred = tf.argmax(y_pred, axis=-1)\n    y_pred = tf.cast(y_pred, y_true.dtype)\n\n    match = tf.cast(y_true == y_pred, tf.float32)\n    mask = tf.cast(y_true != 0, tf.float32)\n\n    return tf.reduce_sum(match)/tf.reduce_sum(mask)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.108437Z","iopub.status.idle":"2023-12-10T01:50:05.108840Z","shell.execute_reply.started":"2023-12-10T01:50:05.108640Z","shell.execute_reply":"2023-12-10T01:50:05.108659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=masked_loss, \n              metrics=[masked_acc, masked_loss])","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.110271Z","iopub.status.idle":"2023-12-10T01:50:05.110706Z","shell.execute_reply.started":"2023-12-10T01:50:05.110511Z","shell.execute_reply":"2023-12-10T01:50:05.110531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 1.0 * french_vectorizer.vocabulary_size()\n\n{\n    \"expected_loss\": tf.math.log(vocab_size).numpy(),\n    \"expected_acc\": 1/vocab_size\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.112121Z","iopub.status.idle":"2023-12-10T01:50:05.112541Z","shell.execute_reply.started":"2023-12-10T01:50:05.112344Z","shell.execute_reply":"2023-12-10T01:50:05.112364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This should roughly match the values returned by running a few steps of evaluation:","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_ds, steps=20, return_dict=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.113826Z","iopub.status.idle":"2023-12-10T01:50:05.115217Z","shell.execute_reply.started":"2023-12-10T01:50:05.114965Z","shell.execute_reply":"2023-12-10T01:50:05.114998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_ds.repeat(), # .repeat() makes it an infinite dataset\n    validation_data=test_ds,\n    epochs=20,\n    steps_per_epoch = 100, # since we are using an infinite dataset, we need to specify the number of steps per epoch\n    validation_steps = 20,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(patience=3)\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.116248Z","iopub.status.idle":"2023-12-10T01:50:05.116669Z","shell.execute_reply.started":"2023-12-10T01:50:05.116478Z","shell.execute_reply":"2023-12-10T01:50:05.116497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"6-inference\"></a>\n# 6. Inference","metadata":{}},{"cell_type":"code","source":"@Translator.add_method\ndef translate(self,\n              texts, *,\n              max_length=50,\n              temperature=0.0):\n    # Process the input texts\n    context = self.encoder.encode_text(texts)\n    batch_size = tf.shape(texts)[0]\n\n    # Setup the loop inputs\n    tokens = []\n    next_token, done, state = self.decoder.get_initial_state(context)\n\n    for _ in range(max_length):\n        # Generate the next token\n        next_token, done, state = self.decoder.get_next_token(context, next_token, done,  state, temperature)\n\n        # Collect the generated tokens\n        tokens.append(next_token)\n\n        if tf.executing_eagerly() and tf.reduce_all(done):\n            break\n\n    # Stack the lists of tokens and attention weights.\n    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n\n    result = self.decoder.tokens_to_text(tokens)\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.117879Z","iopub.status.idle":"2023-12-10T01:50:05.118272Z","shell.execute_reply.started":"2023-12-10T01:50:05.118082Z","shell.execute_reply":"2023-12-10T01:50:05.118101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try it out:\nresult = model.translate([\"This is a wonderful day\"]) # Câ€™est un jour merveilleux\nresult[0].numpy().decode()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:50:05.122580Z","iopub.status.idle":"2023-12-10T01:50:05.123014Z","shell.execute_reply.started":"2023-12-10T01:50:05.122812Z","shell.execute_reply":"2023-12-10T01:50:05.122832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TOC --><a name=\"7-conclusion\"></a>\n# 7. Conclusion\n\nIn this notebook, we used the Encoder-Decoder architecture with Attention to translate English text to French text.\n\nðŸ˜Š If you enjoyed this notebook or found it inspiring/useful, an upvote would be really appreciated.","metadata":{}}]}